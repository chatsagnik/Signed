# -*- coding: utf-8 -*-
"""MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gzt-4EnottTbfRpKICWFojshaEsAQn4M
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import numpy as np
import pandas as pd
import time
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import ComplementNB
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import os

from google.colab import drive
drive.mount('/content/drive')



datapath = ""

def set_Datapath(paths):
	if os.path.isfile(paths):
  		datapath = paths

set_Datapath('C:/Users/Sagnik.DESKTOP-L7KKBDT/Desktop/ML_project/Final Code/model_training')

X_train = pd.read_csv(datapath+'X_train.csv')
X_test = pd.read_csv(datapath+'X_test.csv')
y_train = pd.read_csv(datapath+'y_train.csv')
y_test = pd.read_csv(datapath+'y_test.csv')

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
from sklearn.decomposition import PCA
pca = PCA().fit(X_train)
plt.plot(np.cumsum(pca.explained_variance_ratio_),'k-',linewidth=2)
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.3),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.4),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.5),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.6),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.7),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.8),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.9),'g--',linewidth=0.5)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.95),'r--',linewidth=0.75)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 0.99),'r--',linewidth=0.75)
plt.plot([0,100,200,300,400,500,600,700,800],np.full(9, 1.0),'g--',linewidth=0.5)

plt.plot(np.full(8, 0),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 100),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 110),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'r--',linewidth=0.75)
plt.plot(np.full(8, 200),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 250),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'r--',linewidth=0.75)
plt.plot(np.full(8, 300),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 400),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 500),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 600),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 700),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)
plt.plot(np.full(8, 784),[0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],'g--',linewidth=0.5)

from sklearn.decomposition import TruncatedSVD
svd = TruncatedSVD(random_state=42).fit(X_train)
plt.plot(np.cumsum(svd.explained_variance_ratio_),'r-')
plt.xlabel('Number of Components')
plt.ylabel('Cumulative Explained Variance Ratio')

gnb = GaussianNB()

start = time.time()
gnb.fit(X_train, y_train)
end =  time.time()
print("Time to train Gaussian Naive Bayes model: ",end-start)
y_pred = gnb.predict(X_train)
print("Training Accuracy: ", accuracy_score(y_train, y_pred))

y_pred = gnb.predict(X_test)
print("Test Accuracy: ", accuracy_score(y_test, y_pred))

mnb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)

start = time.time()
mnb.fit(X_train, y_train)
end =  time.time()
print("Time to train Multinomial Naive Bayes model: ",end-start)
y_pred = mnb.predict(X_train)
print("Training Accuracy: ", accuracy_score(y_train, y_pred))

y_pred = mnb.predict(X_test)
print("Test Accuracy: ", accuracy_score(y_test, y_pred))

cnb = ComplementNB(alpha=1.0, class_prior=None, fit_prior=True)

start = time.time()
cnb.fit(X_train, y_train)
end =  time.time()
print("Time to train Complement Naive Bayes model: ",end-start)
y_pred = cnb.predict(X_train)
print("Training Accuracy: ", accuracy_score(y_train, y_pred))

y_pred = cnb.predict(X_test)
print("Test Accuracy: ", accuracy_score(y_test, y_pred))



logisticRegr = LogisticRegression()

##‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.
##‘elasticnet’ is only supported by the ‘saga’ solver.
##If ‘none’ (not supported by the liblinear solver), no regularization is applied.

parameters = [ {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['newton-cg', 'lbfgs'],'penalty':['l2','none'],'class_weight':['balanced','none']},
               #{'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['saga'],'penalty':['l1','l2','elasticnet'],'class_weight':['balanced','none']},
               {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],'solver':['liblinear'],'penalty':['l1','l2']}
            ]
	  
grid_search = GridSearchCV(estimator = logisticRegr, param_grid = parameters, scoring = 'accuracy', cv = 3, n_jobs = -1, verbose = 10)
grid_search = grid_search.fit(X_train, y_train)

accuracy = grid_search.best_score_
print(accuracy)

dict_params = grid_search.best_params_
print(dict_params)

logisticRegr = LogisticRegression(C= 1000, class_weight= 'none', penalty= 'l2', solver= 'newton-cg')

start = time.time()
logisticRegr.fit(X_train, y_train)
end =  time.time()
print("Time to train Logistic Regression model: ",end-start)

y_pred = logisticRegr.predict(X_train)
print("Training Accuracy: ", accuracy_score(y_train, y_pred))

y_pred = logisticRegr.predict(X_test)
print("Test Accuracy: ", accuracy_score(y_test, y_pred))

decisionTree = DecisionTreeClassifier()

parameters=[{'criterion':['gini','entropy'],'max_depth':[5,7,9,11,14],'splitter':['best','random']}]

grid_search = GridSearchCV(estimator = decisionTree, param_grid = parameters, scoring = 'accuracy', cv = 3, n_jobs = -1, verbose = 10)
grid_search = grid_search.fit(X_train, y_train)

accuracy = grid_search.best_score_
print(accuracy)

dict_params = grid_search.best_params_
print(dict_params)

decisionTree = DecisionTreeClassifier(criterion = 'gini', max_depth = 14,splitter='best')

start = time.time()
decisionTree.fit(X_train, y_train)
end =  time.time()
print("Time to train Decision Tree Classifier model: ",end-start)

y_pred = decisionTree.predict(X_train)
print("Training Accuracy: ", accuracy_score(y_train, y_pred))

y_pred = decisionTree.predict(X_test)
print("Test Accuracy: ", accuracy_score(y_test, y_pred))